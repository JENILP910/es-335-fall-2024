{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2 \n",
    "\n",
    "**Total marks: 10 (This assignment total to 20, we will overall scale by a factor of 0.5)**\n",
    "\n",
    "> ## Task 1 : Ascending the Gradient Descent [6 marks]\n",
    "\n",
    "> Use the below dataset for Task 1: \n",
    "``` python\n",
    "np.random.seed(45)\n",
    "num_samples = 40\n",
    "    \n",
    "# Generate data\n",
    "x1 = np.random.uniform(-1, 1, num_samples)\n",
    "f_x = 3*x1 + 4\n",
    "eps = np.random.randn(num_samples)\n",
    "y = f_x + eps\n",
    "```\n",
    "\n",
    "> 1. Use ```torch.autograd``` to find the true gradient on the above dataset using linear regression (in the form $\\theta_1x + \\theta_0$) for any given values of $(\\theta_0,\\theta_1)$. **[1 mark]**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient with respect to theta_0: -7.447054386138916\n",
      "Gradient with respect to theta_1: -1.0253016948699951\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Set seed and generate data\n",
    "np.random.seed(45)\n",
    "num_samples = 40\n",
    "\n",
    "# Generate data\n",
    "x1 = np.random.uniform(-1, 1, num_samples)\n",
    "f_x = 3 * x1 + 4\n",
    "eps = np.random.randn(num_samples)\n",
    "y = f_x + eps\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "x1_tensor = torch.tensor(x1, dtype=torch.float32).view(-1, 1)\n",
    "y_tensor = torch.tensor(y, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "# Define the linear regression model\n",
    "def linear_regression(x, theta_0, theta_1):\n",
    "    return theta_1 * x + theta_0\n",
    "\n",
    "# Initialize parameters\n",
    "theta_0 = torch.tensor(0.0, requires_grad=True)  # Intercept\n",
    "theta_1 = torch.tensor(0.0, requires_grad=True)  # Slope\n",
    "\n",
    "# Forward pass\n",
    "y_pred = linear_regression(x1_tensor, theta_0, theta_1)\n",
    "\n",
    "# Define the loss (Mean Squared Error)\n",
    "loss = torch.mean((y_pred - y_tensor) ** 2)\n",
    "\n",
    "# Backward pass to compute gradients\n",
    "loss.backward()\n",
    "\n",
    "# Print gradients\n",
    "print(f\"Gradient with respect to theta_0: {theta_0.grad.item()}\")\n",
    "print(f\"Gradient with respect to theta_1: {theta_1.grad.item()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 2. Using the same $(\\theta_0,\\theta_1)$ as above, calculate the stochastic gradient for all points in the dataset. Then, find the average of all those gradients and show that the stochastic gradient is a good estimate of the true gradient.  **[1 mark]**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset generation\n",
    "def get_dataset():\n",
    "    np.random.seed(45)\n",
    "    num_samples = 40\n",
    "    x1 = np.random.uniform(-1, 1, num_samples)\n",
    "    f_x = 3 * x1 + 4\n",
    "    eps = np.random.randn(num_samples)\n",
    "    y = f_x + eps\n",
    "\n",
    "    x_train = torch.tensor(x1, dtype=torch.float32).unsqueeze(1)\n",
    "    y_train = torch.tensor(y, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "    return x_train, y_train\n",
    "\n",
    "x_train, y_train = get_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Grad Values - θ0: -8.68634033203125, θ1: -0.6147982478141785\n",
      "Stochastic Grad Values - θ0: -8.686340588331223, θ1: -0.6147983506321907\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Initialize global parameters with requires_grad=True\n",
    "theta_0 = torch.randn(1, requires_grad=True, dtype=torch.float32)\n",
    "theta_1 = torch.randn(1, requires_grad=True, dtype=torch.float32)\n",
    "\n",
    "# Define the linear regression model\n",
    "def linear_regression_model(x):\n",
    "    return theta_1 * x + theta_0\n",
    "\n",
    "# Define the MSE loss function\n",
    "def mse_loss(y_true, y_pred):\n",
    "    return torch.mean((y_pred - y_true) ** 2)\n",
    "\n",
    "# Compute true gradients\n",
    "y_pred = linear_regression_model(x_train)\n",
    "loss = mse_loss(y_train, y_pred)\n",
    "loss.backward()\n",
    "\n",
    "true_grad_values = (theta_0.grad.item(), theta_1.grad.item())\n",
    "\n",
    "# Stochastic gradient function\n",
    "def stochastic_gradient(x_train, y_train):\n",
    "    stoch_grad_theta_0 = []\n",
    "    stoch_grad_theta_1 = []\n",
    "    \n",
    "    # Use the same global parameters\n",
    "    global theta_0, theta_1\n",
    "\n",
    "    for i in range(x_train.shape[0]):\n",
    "        # Create single data point tensors\n",
    "        x_single = x_train[i].unsqueeze(0)  \n",
    "        y_single = y_train[i].unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "        # Forward pass\n",
    "        y_pred_i = linear_regression_model(x_single)\n",
    "        loss = mse_loss(y_single, y_pred_i)\n",
    "        \n",
    "        # Zero gradients\n",
    "        if theta_0.grad is not None:\n",
    "            theta_0.grad.data.zero_()\n",
    "        if theta_1.grad is not None:\n",
    "            theta_1.grad.data.zero_()\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Store gradients\n",
    "        stoch_grad_theta_0.append(theta_0.grad.item())\n",
    "        stoch_grad_theta_1.append(theta_1.grad.item())\n",
    "    \n",
    "    avg_theta_0 = sum(stoch_grad_theta_0) / len(stoch_grad_theta_0)\n",
    "    avg_theta_1 = sum(stoch_grad_theta_1) / len(stoch_grad_theta_1)\n",
    "    \n",
    "    return avg_theta_0, avg_theta_1\n",
    "\n",
    "# Compute stochastic gradients\n",
    "stochastic_grad_values = stochastic_gradient(x_train, y_train)\n",
    "\n",
    "#Results\n",
    "print(f\"True Grad Values - θ0: {true_grad_values[0]}, θ1: {true_grad_values[1]}\")\n",
    "print(f\"Stochastic Grad Values - θ0: {stochastic_grad_values[0]}, θ1: {stochastic_grad_values[1]}\")\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "  As we can see from the values of true gradient and sctochastic gradient values they are nearly same upto 6 decimal places.\n",
    "  So we can estimate that stochastic gradient is nearly a good estimate of true gradient.\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
